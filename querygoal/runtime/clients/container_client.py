"""
Container Client - Docker ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ (Docker-only)
"""
import asyncio
import json
import logging
import shutil
import uuid
from typing import Dict, Any
from pathlib import Path
from datetime import datetime

from ..exceptions import SimulationExecutionError

logger = logging.getLogger("querygoal.container_client")


class ContainerClient:
    """ì»¨í…Œì´ë„ˆ ì‹¤í–‰ í´ë¼ì´ì–¸íŠ¸ (Docker-only)"""

    def __init__(self, execution_mode: str = "docker"):
        self.execution_mode = execution_mode  # "docker" only

    async def run_simulation(self,
                           image: str,
                           input_data: Dict[str, Any],
                           work_directory: Path,
                           goal_id: str) -> Dict[str, Any]:
        """ì‹œë®¬ë ˆì´ì…˜ ì»¨í…Œì´ë„ˆ ì‹¤í–‰"""

        execution_id = f"{goal_id}_{uuid.uuid4().hex[:8]}"
        start_time = datetime.utcnow()

        logger.info(f"ğŸš€ Starting simulation container: {image}")
        logger.info(f"ğŸ“‹ Execution ID: {execution_id}")

        try:
            if self.execution_mode == "docker":
                result = await self._run_docker_container(
                    image, input_data, work_directory, execution_id
                )
            else:
                raise SimulationExecutionError(
                    "Kubernetes execution is not currently supported. Use Docker (execution_mode=docker)"
                )

            execution_time = (datetime.utcnow() - start_time).total_seconds()

            result.update({
                "execution_id": execution_id,
                "execution_time": execution_time,
                "start_time": start_time.isoformat(),
                "end_time": datetime.utcnow().isoformat()
            })

            logger.info(f"âœ… Simulation completed in {execution_time:.2f}s")
            return result

        except Exception as e:
            execution_time = (datetime.utcnow() - start_time).total_seconds()
            logger.error(f"âŒ Simulation failed after {execution_time:.2f}s: {e}")
            raise SimulationExecutionError(f"Container execution failed: {e}") from e

    async def _run_docker_container(self,
                                  image: str,
                                  input_data: Dict[str, Any],
                                  work_directory: Path,
                                  execution_id: str) -> Dict[str, Any]:
        """Docker ì»¨í…Œì´ë„ˆ ì‹¤í–‰"""

        try:
            # Goal3 ì‹œë‚˜ë¦¬ì˜¤ ë””ë ‰í„°ë¦¬ ì¤€ë¹„
            # Docker ì´ë¯¸ì§€ê°€ ê¸°ë³¸ê°’ìœ¼ë¡œ "my_case"ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ì— ë§ì¶¤
            scenario_name = "my_case"
            scenario_dir = work_directory / scenario_name
            scenario_dir.mkdir(exist_ok=True)

            logger.info(f"ğŸ“ Creating scenario directory: {scenario_dir}")

            # íŒŒì¼ ë§¤í•‘ ì¤€ë¹„ (yamlBinding ì¶œë ¥ -> Docker ì»¨í…Œì´ë„ˆ ê¸°ëŒ€ í˜•ì‹)
            file_mappings = {
                "JobOrders": "jobs.json",
                "Machines": "machines.json",
                # ì¶”ê°€ í•„ìˆ˜ íŒŒì¼ë“¤ì€ ê¸°ë³¸ê°’ìœ¼ë¡œ ìƒì„±
                "operations": "operations.json",
                "operation_durations": "operation_durations.json",
                "machine_transfer_time": "machine_transfer_time.json",
                "job_release": "job_release.json"
            }

            # yamlBindingì—ì„œ ìƒì„±ëœ íŒŒì¼ë“¤ ë³µì‚¬ ë° ì´ë¦„ ë³€ê²½
            data_files = input_data.get("data_files", {})
            for source_name, target_name in file_mappings.items():
                if source_name in data_files:
                    # yamlBinding íŒŒì¼ ë³µì‚¬ ë° ì´ë¦„ ë³€ê²½
                    source_path = Path(data_files[source_name])
                    if source_path.exists():
                        target_path = scenario_dir / target_name
                        shutil.copy2(source_path, target_path)
                        logger.info(f"ğŸ“„ Copied {source_name}.json -> {target_name}")
                elif target_name in ["operations.json", "operation_durations.json",
                                     "machine_transfer_time.json", "job_release.json"]:
                    # í•„ìˆ˜ íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ìƒì„±
                    target_path = scenario_dir / target_name
                    await self._create_default_scenario_file(target_name, target_path, data_files)
                    logger.info(f"ğŸ“„ Created default {target_name}")

            # ê²°ê³¼ ë””ë ‰í„°ë¦¬ ì¤€ë¹„
            results_dir = work_directory / "results"
            results_dir.mkdir(exist_ok=True)

            # Docker ì‹¤í–‰ ëª…ë ¹ì–´ êµ¬ì„± (ì‹œë‚˜ë¦¬ì˜¤ ë””ë ‰í„°ë¦¬ë¥¼ ë³¼ë¥¨ ë§ˆìš´íŠ¸)
            docker_cmd = [
                "docker", "run",
                "--rm",  # ì»¨í…Œì´ë„ˆ ìë™ ì‚­ì œ
                "-v", f"{scenario_dir}:/app/scenarios/{scenario_name}",  # ì‹œë‚˜ë¦¬ì˜¤ ë³¼ë¥¨ ë§ˆìš´íŠ¸
                "-v", f"{results_dir}:/app/results",  # ê²°ê³¼ ë””ë ‰í„°ë¦¬ ë§ˆìš´íŠ¸
                "-v", f"{work_directory}:/workspace",  # ì‘ì—… ë””ë ‰í„°ë¦¬ ë§ˆìš´íŠ¸
                "--name", f"simulation-{execution_id}",
                "-e", f"SCENARIO_NAME={scenario_name}",  # ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ í™˜ê²½ë³€ìˆ˜
                "-e", f"TIME_LIMIT=300",  # ì‹œê°„ ì œí•œ
                "-e", f"MAX_NODES=100000",  # ìµœëŒ€ ë…¸ë“œ ìˆ˜
                "-e", f"RESULT_PATH=/app/results"  # ê²°ê³¼ ê²½ë¡œ
            ]

            # ì¶”ê°€ íŒŒë¼ë¯¸í„° í™˜ê²½ ë³€ìˆ˜ë¡œ ì „ë‹¬
            for key, value in input_data.get("parameters", {}).items():
                docker_cmd.extend(["-e", f"{key.upper()}={value}"])

            # ì´ë¯¸ì§€ ì´ë¦„ì€ ë§ˆì§€ë§‰ì— ì¶”ê°€
            docker_cmd.append(image)

            logger.info(f"ğŸ³ Docker command: {' '.join(docker_cmd)}")

            # ë¹„ë™ê¸° í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
            process = await asyncio.create_subprocess_exec(
                *docker_cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=work_directory
            )

            stdout, stderr = await process.communicate()

            # ê²°ê³¼ ì €ì¥
            logs_file = work_directory / f"container_logs_{execution_id}.txt"
            with open(logs_file, 'w', encoding='utf-8') as f:
                f.write(f"=== STDOUT ===\n{stdout.decode('utf-8', errors='replace')}\n")
                f.write(f"=== STDERR ===\n{stderr.decode('utf-8', errors='replace')}\n")

            if process.returncode != 0:
                raise SimulationExecutionError(
                    f"Docker container failed with exit code {process.returncode}: "
                    f"{stderr.decode('utf-8', errors='replace')}"
                )

            # ì¶œë ¥ ê²°ê³¼ íŒŒì‹± ì‹œë„
            output_data = {}
            try:
                stdout_str = stdout.decode('utf-8', errors='replace')
                for line in stdout_str.split('\n'):
                    line = line.strip()
                    if line.startswith('{') and line.endswith('}'):
                        output_data = json.loads(line)
                        break
            except json.JSONDecodeError:
                output_data = {"raw_output": stdout.decode('utf-8', errors='replace')}

            return {
                "execution_mode": "docker",
                "container_image": image,
                "exit_code": process.returncode,
                "output": output_data,
                "logs_path": str(logs_file)
            }

        except Exception as e:
            raise SimulationExecutionError(f"Docker execution failed: {e}") from e

    async def _create_default_scenario_file(self,
                                           file_name: str,
                                           target_path: Path,
                                           data_files: Dict[str, Any]):
        """Goal3 ì‹œë®¬ë ˆì´ì…˜ì— í•„ìš”í•œ ê¸°ë³¸ ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ ìƒì„±"""

        # JobOrdersì™€ Machines ë°ì´í„° ë¡œë“œ
        jobs_data = []
        machines_data = []

        if "JobOrders" in data_files:
            jobs_path = Path(data_files["JobOrders"])
            if jobs_path.exists():
                with open(jobs_path, 'r', encoding='utf-8') as f:
                    jobs_data = json.load(f)

        if "Machines" in data_files:
            machines_path = Path(data_files["Machines"])
            if machines_path.exists():
                with open(machines_path, 'r', encoding='utf-8') as f:
                    machines_data = json.load(f)

        # íŒŒì¼ë³„ ê¸°ë³¸ ë°ì´í„° ìƒì„±
        default_data = {}

        if file_name == "operations.json":
            # JobOrdersì—ì„œ operations ì¶”ì¶œ
            operations = set()
            for job in jobs_data:
                for op in job.get("operations", []):
                    operations.add(op)
            default_data = list(operations) if operations else ["drilling", "welding", "testing", "painting"]

        elif file_name == "operation_durations.json":
            # ê¸°ë³¸ operation duration ë§¤íŠ¸ë¦­ìŠ¤
            operations = ["drilling", "welding", "testing", "painting"]
            machine_types = ["CNC", "WeldingRobot", "VisionInspector", "PaintingRobot"]
            duration_matrix = {}

            for op in operations:
                duration_matrix[op] = {}
                for m_type in machine_types:
                    # íŠ¹ì • ë¨¸ì‹ ì´ íŠ¹ì • ì‘ì—…ì— íŠ¹í™”ëœ ì‹œê°„ ì„¤ì •
                    if (op == "drilling" and m_type == "CNC") or \
                       (op == "welding" and m_type == "WeldingRobot") or \
                       (op == "testing" and m_type == "VisionInspector") or \
                       (op == "painting" and m_type == "PaintingRobot"):
                        duration_matrix[op][m_type] = 5  # íŠ¹í™” ì‘ì—…ì€ ë¹ ë¦„
                    else:
                        duration_matrix[op][m_type] = 99999  # ë¶ˆê°€ëŠ¥í•œ ì‘ì—…

            default_data = duration_matrix

        elif file_name == "machine_transfer_time.json":
            # ë¨¸ì‹  ê°„ ì´ë™ ì‹œê°„ ë§¤íŠ¸ë¦­ìŠ¤
            machine_ids = [m["id"] for m in machines_data] if machines_data else ["M1", "M2", "M3", "M4"]
            transfer_matrix = {}

            for m1 in machine_ids:
                transfer_matrix[m1] = {}
                for m2 in machine_ids:
                    if m1 == m2:
                        transfer_matrix[m1][m2] = 0
                    else:
                        # ì¸ì ‘ ë¨¸ì‹ ì€ 1, ë©€ë¦¬ ìˆëŠ” ë¨¸ì‹ ì€ 2-3
                        m1_idx = machine_ids.index(m1)
                        m2_idx = machine_ids.index(m2)
                        distance = abs(m1_idx - m2_idx)
                        transfer_matrix[m1][m2] = min(distance, 3)

            default_data = transfer_matrix

        elif file_name == "job_release.json":
            # Job release ì‹œê°„ ì •ë³´
            release_times = {}
            for i, job in enumerate(jobs_data):
                job_id = job.get("job_id", f"JOB{i:03d}")
                # ìˆœì°¨ì ìœ¼ë¡œ release time ì„¤ì •
                release_times[job_id] = i * 2
            default_data = release_times if release_times else {"JOB001": 0, "JOB002": 5}

        # íŒŒì¼ ì €ì¥
        with open(target_path, 'w', encoding='utf-8') as f:
            json.dump(default_data, f, indent=2, ensure_ascii=False)