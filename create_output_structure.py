#!/usr/bin/env python3
"""
Factory Automation K8s - Output Structure Creator
ê° ë‹¨ê³„ë³„ ì‚°ì¶œë¬¼ì„ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ output í´ë”ì— ì €ì¥
"""

import json
import shutil
import requests
import base64
from pathlib import Path
from datetime import datetime
import logging
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

from execution_engine.aasx_data_orchestrator import AASXDataOrchestrator

def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def create_folder_structure():
    """output í´ë” êµ¬ì¡° ìƒì„±"""
    base_path = Path("output")
    
    folders = [
        "step1_aasx_raw_data/input",
        "step1_aasx_raw_data/output", 
        "step2_data_orchestrator/input",
        "step2_data_orchestrator/output",
        "step3_nsga2_simulation/input",
        "step3_nsga2_simulation/output",
        "metadata"
    ]
    
    for folder in folders:
        folder_path = base_path / folder
        folder_path.mkdir(parents=True, exist_ok=True)
        
    return base_path

def collect_aasx_raw_data(base_path: Path, logger):
    """Step 1: AASX ì„œë²„ ì›ë³¸ ë°ì´í„° ìˆ˜ì§‘"""
    logger.info("ğŸ” Step 1: Collecting AASX raw data...")
    
    step1_input = base_path / "step1_aasx_raw_data/input"
    step1_output = base_path / "step1_aasx_raw_data/output"
    
    base_url = "http://127.0.0.1:5001"
    session = requests.Session()
    
    # ì„œë¸Œëª¨ë¸ ë¦¬ìŠ¤íŠ¸
    submodels = [
        {
            "id": "urn:factory:submodel:simulation_data",
            "name": "FactorySimulation", 
            "elements": ["jobs_data", "operations_data", "operation_durations_data", 
                        "machine_transfer_time_data", "job_release_data"]
        },
        {
            "id": "urn:factory:submodel:capability:M1",
            "name": "Machine_M1_Capability",
            "elements": ["machine_type", "efficiency"]
        },
        {
            "id": "urn:factory:submodel:status:M1", 
            "name": "Machine_M1_Status",
            "elements": ["status", "next_available_time", "queue_length"]
        }
    ]
    
    raw_data = {}
    extracted_data = {}
    
    for submodel in submodels:
        submodel_id = submodel["id"]
        submodel_name = submodel["name"]
        
        # Base64 ì¸ì½”ë”©
        encoded_id = base64.urlsafe_b64encode(submodel_id.encode()).decode().rstrip('=')
        
        # ì›ë³¸ ì„œë¸Œëª¨ë¸ ë°ì´í„° ì¡°íšŒ
        try:
            url = f"{base_url}/submodels/{encoded_id}"
            response = session.get(url)
            
            if response.status_code == 200:
                submodel_data = response.json()
                raw_data[submodel_name] = submodel_data
                
                # Property.value í•„ë“œ ì¶”ì¶œ
                extracted_elements = {}
                for element in submodel_data.get('submodelElements', []):
                    element_id = element.get('idShort')
                    if element_id in submodel['elements']:
                        if element.get('modelType') == 'Property' and 'value' in element:
                            extracted_elements[element_id] = element['value']
                
                extracted_data[submodel_name] = extracted_elements
                logger.info(f"  âœ… {submodel_name}: {len(extracted_elements)} elements")
            else:
                logger.warning(f"  âŒ Failed to get {submodel_name}: {response.status_code}")
                
        except Exception as e:
            logger.error(f"  âŒ Error processing {submodel_name}: {e}")
    
    # ì›ë³¸ ë°ì´í„° ì €ì¥
    with open(step1_input / "aasx_raw_responses.json", 'w', encoding='utf-8') as f:
        json.dump(raw_data, f, indent=2, ensure_ascii=False)
    
    # ì¶”ì¶œëœ ë°ì´í„° ì €ì¥  
    with open(step1_output / "extracted_property_values.json", 'w', encoding='utf-8') as f:
        json.dump(extracted_data, f, indent=2, ensure_ascii=False)
    
    logger.info(f"  ğŸ“ Raw data saved: {len(raw_data)} submodels")
    return extracted_data

def organize_orchestrator_data(base_path: Path, logger):
    """Step 2: DataOrchestrator ê²°ê³¼ë¬¼ ì •ë¦¬"""
    logger.info("ğŸ”§ Step 2: Organizing DataOrchestrator outputs...")
    
    step2_input = base_path / "step2_data_orchestrator/input"
    step2_output = base_path / "step2_data_orchestrator/output"
    
    # Step 1 outputì„ Step 2 inputìœ¼ë¡œ ë³µì‚¬
    step1_output = base_path / "step1_aasx_raw_data/output/extracted_property_values.json"
    if step1_output.exists():
        shutil.copy2(step1_output, step2_input / "aasx_extracted_data.json")
    
    # ê¸°ì¡´ ìƒì„±ëœ JSON íŒŒì¼ë“¤ì„ Step 2 outputìœ¼ë¡œ ë³µì‚¬
    source_dir = Path("temp/full_simulation")
    if source_dir.exists():
        for json_file in source_dir.glob("*.json"):
            shutil.copy2(json_file, step2_output / json_file.name)
            logger.info(f"  ğŸ“„ Copied {json_file.name}")
    
    # DataOrchestrator ì‹¤í–‰ ë¡œê·¸ ìƒì„±
    orchestrator_log = {
        "timestamp": datetime.now().isoformat(),
        "input_source": "AASX Server Property.value fields",
        "processing_method": "Direct submodel access",
        "generated_files": [f.name for f in step2_output.glob("*.json")],
        "file_sizes": {f.name: f.stat().st_size for f in step2_output.glob("*.json")}
    }
    
    with open(step2_output / "orchestrator_execution_log.json", 'w', encoding='utf-8') as f:
        json.dump(orchestrator_log, f, indent=2, ensure_ascii=False)
    
    logger.info(f"  ğŸ“ Orchestrator data organized: {len(list(step2_output.glob('*.json')))} files")

def organize_simulation_data(base_path: Path, logger):
    """Step 3: NSGA-II ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ì •ë¦¬"""
    logger.info("ğŸš€ Step 3: Organizing NSGA-II simulation data...")
    
    step3_input = base_path / "step3_nsga2_simulation/input" 
    step3_output = base_path / "step3_nsga2_simulation/output"
    
    # Step 2 outputì„ Step 3 inputìœ¼ë¡œ ë³µì‚¬ (ì‹œë®¬ë ˆì´ì…˜ ì…ë ¥ íŒŒì¼ë“¤)
    step2_output = base_path / "step2_data_orchestrator/output"
    simulation_files = ["jobs.json", "operations.json", "machines.json", 
                       "operation_durations.json", "machine_transfer_time.json", "job_release.json"]
    
    for filename in simulation_files:
        source_file = step2_output / filename
        if source_file.exists():
            shutil.copy2(source_file, step3_input / filename)
    
    # ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ë³µì‚¬
    results_dir = Path("temp/results")
    if results_dir.exists():
        for result_file in results_dir.iterdir():
            if result_file.is_file():
                shutil.copy2(result_file, step3_output / result_file.name)
                logger.info(f"  ğŸ“Š Copied result: {result_file.name}")
    
    # ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ ë©”íƒ€ë°ì´í„° ìƒì„±
    simulation_meta = {
        "timestamp": datetime.now().isoformat(),
        "docker_image": "factory-nsga2:latest",
        "algorithm": "branch_and_bound",
        "scenario": "my_case",
        "input_files": [f.name for f in step3_input.glob("*.json")],
        "output_files": [f.name for f in step3_output.iterdir() if f.is_file()],
        "execution_time_seconds": 33
    }
    
    with open(step3_output / "simulation_metadata.json", 'w', encoding='utf-8') as f:
        json.dump(simulation_meta, f, indent=2, ensure_ascii=False)
    
    logger.info(f"  ğŸ“ Simulation data organized: {len(list(step3_output.iterdir()))} files")

def create_readme_files(base_path: Path):
    """ê° ë‹¨ê³„ë³„ README íŒŒì¼ ìƒì„±"""
    
    # ë©”ì¸ README
    main_readme = """# Factory Automation K8s - Pipeline Output Structure

ì´ í´ë”ëŠ” AASX ì„œë²„ì—ì„œ NSGA-II ì‹œë®¬ë ˆì´ì…˜ê¹Œì§€ì˜ ì „ì²´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê²°ê³¼ë¥¼ ë‹¨ê³„ë³„ë¡œ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤.

## ğŸ“ í´ë” êµ¬ì¡°

```
output/
â”œâ”€â”€ step1_aasx_raw_data/       # AASX ì„œë²„ ì›ë³¸ ë°ì´í„°
â”œâ”€â”€ step2_data_orchestrator/   # DataOrchestrator ì²˜ë¦¬ ê²°ê³¼  
â”œâ”€â”€ step3_nsga2_simulation/    # NSGA-II ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼
â”œâ”€â”€ metadata/                  # ì „ì²´ ì‹¤í–‰ ë©”íƒ€ë°ì´í„°
â””â”€â”€ README.md                  # ì´ íŒŒì¼
```

## ğŸ”„ ë°ì´í„° í”Œë¡œìš°

1. **AASX ì„œë²„** â†’ Property.value í•„ë“œì—ì„œ JSON ë¬¸ìì—´ ì¶”ì¶œ
2. **DataOrchestrator** â†’ 6ê°œ ì‹œë®¬ë ˆì´ì…˜ JSON íŒŒì¼ ìƒì„±
3. **NSGA-II ì‹œë®¬ë ˆì´í„°** â†’ ìŠ¤ì¼€ì¤„ë§ ìµœì í™” ë° ê²°ê³¼ ìƒì„±

ê° ë‹¨ê³„ë³„ ìƒì„¸ ì •ë³´ëŠ” í•´ë‹¹ í´ë”ì˜ README.mdë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
"""

    # Step 1 README
    step1_readme = """# Step 1: AASX Raw Data

## ğŸ“ ì„¤ëª…
AASX ì„œë²„ì—ì„œ ì›ë³¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  Property.value í•„ë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ë‹¨ê³„

## ğŸ“‚ input/
- `aasx_raw_responses.json`: AASX ì„œë²„ ì„œë¸Œëª¨ë¸ ì›ë³¸ ì‘ë‹µ ë°ì´í„°

## ğŸ“‚ output/  
- `extracted_property_values.json`: Property.value í•„ë“œì—ì„œ ì¶”ì¶œëœ JSON ë¬¸ìì—´ë“¤

## ğŸ”§ ì²˜ë¦¬ ë°©ì‹
- ì„œë¸Œëª¨ë¸ ì§ì ‘ ì ‘ê·¼: `GET /submodels/{encoded_id}`
- Property íƒ€ì… í•„í„°ë§
- JSON ë¬¸ìì—´ ì¶”ì¶œ (íŒŒì‹± ì „)
"""

    # Step 2 README  
    step2_readme = """# Step 2: DataOrchestrator Processing

## ğŸ“ ì„¤ëª…
AASX ì„œë²„ì—ì„œ ì¶”ì¶œëœ ë°ì´í„°ë¥¼ NSGA-II ì‹œë®¬ë ˆì´ì…˜ìš© JSON íŒŒì¼ë¡œ ë³€í™˜í•˜ëŠ” ë‹¨ê³„

## ğŸ“‚ input/
- `aasx_extracted_data.json`: Step 1ì—ì„œ ì¶”ì¶œëœ Property.value ë°ì´í„°

## ğŸ“‚ output/
- `jobs.json`: ì‘ì—… ì •ë³´ (30ê°œ ì‘ì—…)
- `operations.json`: ì˜¤í¼ë ˆì´ì…˜ ì •ë³´ (95ê°œ ì˜¤í¼ë ˆì´ì…˜)  
- `machines.json`: ë¨¸ì‹  ì •ë³´ (4ê°œ ë¨¸ì‹ )
- `operation_durations.json`: ì‘ì—… ì†Œìš” ì‹œê°„
- `machine_transfer_time.json`: ë¨¸ì‹  ê°„ ì´ë™ ì‹œê°„
- `job_release.json`: ì‘ì—… ë¦´ë¦¬ì¦ˆ ì‹œê°„
- `orchestrator_execution_log.json`: ì‹¤í–‰ ë¡œê·¸

## ğŸ”§ ì²˜ë¦¬ ë°©ì‹
- JSON ë¬¸ìì—´ íŒŒì‹±
- ì‹œë®¬ë ˆì´í„° ìš”êµ¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
- ë¨¸ì‹ ë³„ capability/status ë°ì´í„° í†µí•©
"""

    # Step 3 README
    step3_readme = """# Step 3: NSGA-II Simulation

## ğŸ“ ì„¤ëª…  
DataOrchestratorì—ì„œ ìƒì„±ëœ JSON íŒŒì¼ë“¤ì„ ì‚¬ìš©í•´ NSGA-II ì‹œë®¬ë ˆì´ì…˜ì„ ì‹¤í–‰í•˜ëŠ” ë‹¨ê³„

## ğŸ“‚ input/
- `jobs.json`, `operations.json`, `machines.json` ë“±: ì‹œë®¬ë ˆì´ì…˜ ì…ë ¥ íŒŒì¼ë“¤

## ğŸ“‚ output/
- `goal3_manifest.json`: ì‹¤í–‰ ë©”íƒ€ë°ì´í„°
- `simulator_optimization_result.json`: ìµœì í™” ê²°ê³¼
- `job_info.csv`: ì‘ì—… ì •ë³´ ìƒì„¸
- `operation_info.csv`: ì˜¤í¼ë ˆì´ì…˜ ì‹¤í–‰ ì •ë³´
- `agv_logs_*.xlsx`: AGV ë¡œê·¸ íŒŒì¼ë“¤
- `simulation_metadata.json`: ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ ì •ë³´

## ğŸ”§ ì²˜ë¦¬ ë°©ì‹
- Docker ì»¨í…Œì´ë„ˆ: `factory-nsga2:latest`
- ì•Œê³ ë¦¬ì¦˜: branch_and_bound  
- ì‹¤í–‰ ì‹œê°„: ~33ì´ˆ
- ì˜ˆì¸¡ ì™„ë£Œ ì‹œê°„: 3600ì´ˆ
"""

    # README íŒŒì¼ë“¤ ì €ì¥
    readme_files = [
        (base_path / "README.md", main_readme),
        (base_path / "step1_aasx_raw_data/README.md", step1_readme), 
        (base_path / "step2_data_orchestrator/README.md", step2_readme),
        (base_path / "step3_nsga2_simulation/README.md", step3_readme)
    ]
    
    for filepath, content in readme_files:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)

def create_metadata(base_path: Path):
    """ì „ì²´ ì‹¤í–‰ ë©”íƒ€ë°ì´í„° ìƒì„±"""
    metadata = {
        "pipeline_execution": {
            "timestamp": datetime.now().isoformat(),
            "total_steps": 3,
            "success": True,
            "execution_environment": {
                "aasx_server": "http://127.0.0.1:5001",
                "docker_image": "factory-nsga2:latest", 
                "python_version": sys.version
            }
        },
        "data_flow_summary": {
            "step1_aasx_raw": "Extract Property.value from AASX server submodels",
            "step2_orchestrator": "Convert extracted data to 6 simulation JSON files", 
            "step3_simulation": "Run NSGA-II optimization with generated files"
        },
        "file_counts": {
            "step1_input": len(list((base_path / "step1_aasx_raw_data/input").glob("*"))),
            "step1_output": len(list((base_path / "step1_aasx_raw_data/output").glob("*"))),
            "step2_input": len(list((base_path / "step2_data_orchestrator/input").glob("*"))),
            "step2_output": len(list((base_path / "step2_data_orchestrator/output").glob("*"))),
            "step3_input": len(list((base_path / "step3_nsga2_simulation/input").glob("*"))),
            "step3_output": len(list((base_path / "step3_nsga2_simulation/output").glob("*")))
        }
    }
    
    metadata_file = base_path / "metadata/pipeline_execution_metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger = setup_logging()
    
    logger.info("ğŸ—ï¸  Creating Factory Automation K8s Output Structure...")
    
    try:
        # 1. í´ë” êµ¬ì¡° ìƒì„±
        base_path = create_folder_structure()
        logger.info(f"ğŸ“ Created output structure at: {base_path.absolute()}")
        
        # 2. AASX ì›ë³¸ ë°ì´í„° ìˆ˜ì§‘
        collect_aasx_raw_data(base_path, logger)
        
        # 3. DataOrchestrator ë°ì´í„° ì •ë¦¬
        organize_orchestrator_data(base_path, logger)
        
        # 4. ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ì •ë¦¬  
        organize_simulation_data(base_path, logger)
        
        # 5. README íŒŒì¼ë“¤ ìƒì„±
        create_readme_files(base_path)
        logger.info("ğŸ“– Created README files for all steps")
        
        # 6. ë©”íƒ€ë°ì´í„° ìƒì„±
        create_metadata(base_path)
        logger.info("ğŸ“Š Created pipeline metadata")
        
        logger.info("âœ… Output structure creation completed successfully!")
        logger.info(f"ğŸ“‚ Check results in: {base_path.absolute()}")
        
    except Exception as e:
        logger.error(f"âŒ Failed to create output structure: {e}")
        raise

if __name__ == "__main__":
    main()